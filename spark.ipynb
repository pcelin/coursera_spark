{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23668893-d4a8-47d3-8e3f-0885c4de1cd6",
   "metadata": {},
   "source": [
    "# Spark, Hadoop, and Snowflake for Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc9cece-ff10-4c10-af09-5e49b172e196",
   "metadata": {},
   "source": [
    "## Hadoop $\\rightarrow$ distributed computation and storage and writes data to disk\n",
    "Hadoop might be prefered for batch processing and might me o good choice if you have a Hadoop Cluest in place.\n",
    "\n",
    "## Spark $\\rightarrow$ distributed computation, keeps data in-memory\n",
    "Spark can be usefull for real-time processing. For fresh-starters, might be a good choice because of its easier API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800c2994-3441-418a-b6de-f591ca0d1e23",
   "metadata": {},
   "source": [
    "## Resiliece Distributed Dataset\n",
    "\n",
    "This is the sequency:\n",
    "\n",
    "Dataset $\\rightarrow$ RDD 1 (Transformation 1) $\\rightarrow$ RDD 2 (Transformation 2) $\\rightarrow$ RDD 3 (ACTION) $\\rightarrow$ RDD 4 $\\rightarrow$ Dataset (on disk)\n",
    "\n",
    "Transformations will only be executed after an action.\n",
    "\n",
    "Shuffle $\\rightarrow$ an operations that makes data to be repartitioned (reorder, etc). This type of opperation is expensive because it writes data to disk. \n",
    "\n",
    "Shuffle $\\rightarrow$ defines the boarders of the Stages of a Job. Stage 1 goes until it finds first shuffle operation and so on.\n",
    "\n",
    "RDDs:\n",
    "- Resilient, distributed across multiple nodes in Spark cluster allowing for parallel processing. Each RDD is divided into partitions, with each partition residing on a separete note.\n",
    "- RDD are imutable, preserving data integrity. Transformations applied to RDDs produces new RDDs\n",
    "- Processed In-Memory\n",
    "- Support two types of operations: transformation (map, filter, reduceByKey) that creates a new RDD and actions (count, collect, saveAsTextFile) shich trigger computations and return results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2982a8b8-46f7-4838-987f-c5e3c0138d3a",
   "metadata": {},
   "source": [
    "## Hands-on\n",
    "\n",
    "Install binary spark file at https://spark.apache.org/downloads.html, go to directory, run ./bin/pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5086087-0fa5-48c5-917c-6193e038bd9d",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "$\\longrightarrow$ Transform SQL queries into MapReduce operations.\n",
    "\n",
    "$\\longrightarrow$ Users can define aggregation and scalar functions with User-Defined Functions (UDFs). \n",
    "\n",
    "$\\longrightarrow$ Is used for working with structured data sets.\n",
    "\n",
    "## Strutucred data set\n",
    "$\\longrightarrow$ Data follows a specific schema, where schema defines the structure, data types and relationships within the dataset\n",
    "\n",
    "$\\longrightarrow$ Used in SQL data bases: table has an schema, defined number of columns of specific types. \n",
    "\n",
    "$\\longrightarrow$ Data can be structured in distributed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7be416e-6357-4d0e-a910-f75ec14888d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mgithub\u001b[0m/  \u001b[01;34mspark-3.5.0-bin-hadoop3\u001b[0m/  \u001b[01;34msparkvenv\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls /home/mpaiva/bin_programs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2b843a-a18f-4079-a2e1-4703d8c4d73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/13 22:42:42 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "23/12/13 22:42:43 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ['SPARK_HOME'] = '/home/mpaiva/bin_programs/spark-3.5.0-bin-hadoop3'  # Replace with your Spark installation path\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python'  # Replace with your Python path if different\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark Example\") \\\n",
    "    .config(\"sparj,some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "695928f5-2d9c-494d-82d7-f30ec85c7948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://celodesk:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>YourAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f127ffe5950>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65efa9-7417-4772-9592-89f8bc468d41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7d464-0fff-44f4-82d7-eb5c4124e939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa81bf43-26df-4ecc-aac1-3a2ce89edd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e61e27-aaa0-4d80-ae54-1e5ff1fbd634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6868491-73d2-44cb-adec-2a86c2542a67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76870832-7da8-4be4-9a20-aa4ac41f931f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a44ef3e-326f-47bb-9beb-eb19b54e9b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7fdfe-543a-4240-bb6e-520b550886a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f712a2a-e66a-45e3-a5b1-e164550361e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bfb429-8af2-4fd9-a261-79b6895825f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ecdd06-a630-49e0-810b-1327b1b95d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2cf778-ee68-4c91-8d9d-20d8ce2aedc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fdefb9-934a-46ad-a805-9ebe2fd935fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7545a7f6-187e-478e-bb0f-a369287fd8f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6f4e72-e500-46df-8cd6-95df62a61c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
